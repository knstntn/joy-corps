{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For requriments, please refer to imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import mixture\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import csv\n",
    "%matplotlib inline\n",
    "#here we define our \"constants\"\n",
    "train_data = r\"/home/yuri/datacombats/data/train\"\n",
    "labels_data = r\"/home/yuri/datacombats/data/train/labels\"\n",
    "predictions_data = r\"/home/yuri/datacombats/data/test/prediction\"\n",
    "test_data = r\"/home/yuri/datacombats/data/test\"\n",
    "\n",
    "#where predictions will be stored. IMPORTANT!!! change the name when changing the model\n",
    "#my convention is \"prediction_{month}_{day}_{count of samples on which GMMs are trained_{strategy for sklearn imputer}\"\n",
    "predictions_save_folder = r\"/home/yuri/datacombats/data/test/prediction_10_30_all_median\"\n",
    "#where predictions will be stored. IMPORTANT!!! change the name when changing the model\n",
    "models_save_folder = r\"/home/yuri/datacombats/data/test/10_30_all_median_models\"\n",
    "#where predictions will be stored. IMPORTANT!!! change the name when changing the model\n",
    "preds_save_path = r\"/home/yuri/datacombats/data/test/10_30_all_median_preds\"\n",
    "\n",
    "if not os.path.exists(predictions_save_folder):\n",
    "    os.mkdir(predictions_save_folder)\n",
    "if not os.path.exists(models_save_folder):\n",
    "    os.mkdir(models_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def replace(s):\n",
    "    t = []\n",
    "    for i in range(0, len(s[:,0])):\n",
    "        t.append(np.sum(s[i, 1:]))\n",
    "        if t[i] == 0:\n",
    "            s[i, 1:] = np.full((1, len(s[i, 1:])), None)\n",
    "    return s\n",
    "\n",
    "def get_data_from_file(path, delimiter = ','):\n",
    "    if os.path.exists(path):\n",
    "        res = genfromtxt(path, delimiter=delimiter, skip_header=True)\n",
    "        res = replace(res)\n",
    "    else:\n",
    "        res = None\n",
    "          \n",
    "    return res\n",
    "\n",
    "def extend(first, second):\n",
    "    import math\n",
    "    a, b = None, None\n",
    "    first_longer = False\n",
    "    #this block makes function symmetrical \n",
    "    if len(first) > len(second):\n",
    "        a = first\n",
    "        b = second\n",
    "        first_longer = True\n",
    "    elif len(first) < len(second):\n",
    "        a = second\n",
    "        b = first\n",
    "    else:\n",
    "        return (first, second)\n",
    "    \n",
    "    b_ext = []\n",
    "    b_ind = 0\n",
    "    for item in a:\n",
    "        #if values in arrs are equal, take current value and inc counter to pick next value next time\n",
    "        if item[0] == b[b_ind][0]:\n",
    "            b_ext.append(b[b_ind])\n",
    "            if b_ind != len(b) - 1:\n",
    "                b_ind += 1\n",
    "        #if value in long arrs is lower than the one we extend, just take the same value as in prev iter\n",
    "        elif item[0] < b[b_ind][0]:\n",
    "            b_ext.append(b[b_ind - 1])\n",
    "        #if bigger than update counter first and take new value\n",
    "        else: \n",
    "            if b_ind != len(b) - 1:\n",
    "                b_ind += 1\n",
    "            b_ext.append(b[b_ind])\n",
    "            \n",
    "    return (a, np.array(b_ext)) if first_longer else (np.array(b_ext),a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we need to load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/306 [00:00<?, ?it/s]\u001b[A\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 306/306 [04:31<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "eyes_len = 6\n",
    "kinect_len = 27\n",
    "audio_len = 36\n",
    "face_len = 100\n",
    "\n",
    "train = None\n",
    "files = []\n",
    "\n",
    "#find unique file names\n",
    "for item in os.listdir(os.path.join(train_data, \"eyes\")):\n",
    "    files.append(item)\n",
    "for item in os.listdir(os.path.join(train_data, \"kinect\")):\n",
    "    files.append(item)\n",
    "for item in os.listdir(os.path.join(train_data, \"audio\")):\n",
    "    files.append(item)\n",
    "for item in os.listdir(os.path.join(train_data, \"face_nn\")):\n",
    "    files.append(item)\n",
    "files = list(set(files))\n",
    "\n",
    "for file in tqdm(files):        \n",
    "    eyes_path = os.path.join(train_data, \"eyes\", file)\n",
    "    kinect_path = os.path.join(train_data, \"kinect\", file)\n",
    "    audio_path = os.path.join(train_data, \"audio\", file)\n",
    "    face_nn_path = os.path.join(train_data, \"face_nn\", file)\n",
    "    \n",
    "    #get all data sources available. \n",
    "    #if for some source file is missing, set to None. If part of file is missing, set to None\n",
    "    #(refer to get_data_from_file func) \n",
    "    eyes_data = get_data_from_file(eyes_path)\n",
    "    audio_data = get_data_from_file(audio_path)\n",
    "    face_nn_data = get_data_from_file(face_nn_path)\n",
    "    kinect_data = get_data_from_file(kinect_path)\n",
    "    #get labels with timestamps, we will need this to align results\n",
    "    labels_raw = genfromtxt(os.path.join(labels_data, file), delimiter=',', skip_header=True)[:, 0:7]\n",
    "    labels_raw[:, 1] = np.dot(labels_raw[:, 1:7], range(0, 6))\n",
    "    labels = labels_raw[:, 0:2]\n",
    "    arr = []\n",
    "    \n",
    "    if eyes_data is not None:\n",
    "        arr.append(eyes_data)\n",
    "    if audio_data is not None:\n",
    "        arr.append(audio_data)\n",
    "    if face_nn_data is not None:\n",
    "        arr.append(face_nn_data)\n",
    "    if kinect_data is not None:\n",
    "        arr.append(kinect_data)\n",
    "    arr.append(labels)\n",
    "        \n",
    "    #find max sampled array and to further use it for other array extending\n",
    "    max_len_arr = max(arr, key=lambda x:len(x))\n",
    "    max_len = len(max_len_arr)\n",
    "    \n",
    "    #replace data with None is data source is missing\n",
    "    if eyes_data is None:\n",
    "        eyes_data = np.hstack((max_len_arr[:, 0].reshape(max_len, 1), np.full((max_len, eyes_len), None)))\n",
    "    if audio_data is None:\n",
    "        audio_data = np.hstack((max_len_arr[:, 0].reshape(max_len, 1), np.full((max_len, audio_len), None)))\n",
    "    if face_nn_data is None:\n",
    "        face_nn_data = np.hstack((max_len_arr[:, 0].reshape(max_len, 1), np.full((max_len, face_len), None)))\n",
    "    if kinect_data is None:\n",
    "        kinect_data = np.hstack((max_len_arr[:, 0].reshape(max_len, 1), np.full((max_len, kinect_len), None)))\n",
    "        \n",
    "    #extend data so every array has equal roq count\n",
    "    eyes_data, _ = extend(eyes_data, max_len_arr)\n",
    "    audio_data, _ = extend(audio_data, max_len_arr)\n",
    "    face_nn_data, _ = extend(face_nn_data, max_len_arr)\n",
    "    kinect_data, _ = extend(kinect_data, max_len_arr)\n",
    "\n",
    "    whole = np.hstack((eyes_data, kinect_data[:, 1:], audio_data[:, 1:], face_nn_data[:, 1:], labels[: , 1:]))\n",
    "    \n",
    "    #add to whole dataset object\n",
    "    if train is None:\n",
    "        train = whole\n",
    "    else:\n",
    "        train = np.vstack((train, whole))\n",
    "        \n",
    "train = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will clean our data to keep only fully filled rows in df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>6.462</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>-0.54717</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.31857</td>\n",
       "      <td>0.240584</td>\n",
       "      <td>0.221185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00279599</td>\n",
       "      <td>0.0210605</td>\n",
       "      <td>-0.0270551</td>\n",
       "      <td>0.0323795</td>\n",
       "      <td>-0.0292688</td>\n",
       "      <td>-0.0133435</td>\n",
       "      <td>-0.0140566</td>\n",
       "      <td>0.0217844</td>\n",
       "      <td>-0.0223791</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>6.442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>-0.54717</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.31857</td>\n",
       "      <td>0.240584</td>\n",
       "      <td>0.221185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0115064</td>\n",
       "      <td>0.0305696</td>\n",
       "      <td>-0.0112508</td>\n",
       "      <td>0.0440542</td>\n",
       "      <td>-0.0245367</td>\n",
       "      <td>-0.0116037</td>\n",
       "      <td>-0.0133617</td>\n",
       "      <td>0.0109437</td>\n",
       "      <td>-0.0194671</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>6.482</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.54717</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.240463</td>\n",
       "      <td>0.319368</td>\n",
       "      <td>0.249078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0072992</td>\n",
       "      <td>0.0290698</td>\n",
       "      <td>-0.0353791</td>\n",
       "      <td>0.0279187</td>\n",
       "      <td>-0.0274823</td>\n",
       "      <td>-0.0195662</td>\n",
       "      <td>-0.0149998</td>\n",
       "      <td>0.0190567</td>\n",
       "      <td>-0.0165976</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>6.462</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>-0.54717</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.283405</td>\n",
       "      <td>0.290645</td>\n",
       "      <td>0.226207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00279599</td>\n",
       "      <td>0.0210605</td>\n",
       "      <td>-0.0270551</td>\n",
       "      <td>0.0323795</td>\n",
       "      <td>-0.0292688</td>\n",
       "      <td>-0.0133435</td>\n",
       "      <td>-0.0140566</td>\n",
       "      <td>0.0217844</td>\n",
       "      <td>-0.0223791</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>6.502</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.54717</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.283405</td>\n",
       "      <td>0.290645</td>\n",
       "      <td>0.226207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0188422</td>\n",
       "      <td>0.0296961</td>\n",
       "      <td>-0.0358609</td>\n",
       "      <td>0.021786</td>\n",
       "      <td>-0.034036</td>\n",
       "      <td>-0.0147312</td>\n",
       "      <td>-0.0142918</td>\n",
       "      <td>0.0169755</td>\n",
       "      <td>-0.0184577</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1   2         3         4        5         6         7         8    \\\n",
       "437  6.462   1   1  0.660377  0.730769 -0.54717  0.942308   0.31857  0.240584   \n",
       "438  6.442   1   0  0.679245  0.730769 -0.54717  0.942308   0.31857  0.240584   \n",
       "439  6.482   1   0  0.679245      0.75 -0.54717  0.923077  0.240463  0.319368   \n",
       "440  6.462   1   1  0.660377  0.730769 -0.54717  0.942308  0.283405  0.290645   \n",
       "441  6.502   1   1  0.679245      0.75 -0.54717  0.942308  0.283405  0.290645   \n",
       "\n",
       "          9   ...         161        162        163        164        165  \\\n",
       "437  0.221185 ... -0.00279599  0.0210605 -0.0270551  0.0323795 -0.0292688   \n",
       "438  0.221185 ...  -0.0115064  0.0305696 -0.0112508  0.0440542 -0.0245367   \n",
       "439  0.249078 ...  -0.0072992  0.0290698 -0.0353791  0.0279187 -0.0274823   \n",
       "440  0.226207 ... -0.00279599  0.0210605 -0.0270551  0.0323795 -0.0292688   \n",
       "441  0.226207 ...  -0.0188422  0.0296961 -0.0358609   0.021786  -0.034036   \n",
       "\n",
       "           166        167        168        169 170  \n",
       "437 -0.0133435 -0.0140566  0.0217844 -0.0223791   4  \n",
       "438 -0.0116037 -0.0133617  0.0109437 -0.0194671   4  \n",
       "439 -0.0195662 -0.0149998  0.0190567 -0.0165976   4  \n",
       "440 -0.0133435 -0.0140566  0.0217844 -0.0223791   4  \n",
       "441 -0.0147312 -0.0142918  0.0169755 -0.0184577   4  \n",
       "\n",
       "[5 rows x 171 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = train.dropna(how='any', axis=0)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will train BayesianGaussianMixture models for every class + imputer to fill empty values again for every class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training  0  emotion\n",
      "Training took  1.3394041061401367\n",
      "Started training  1  emotion\n",
      "Training took  1.1151819229125977\n",
      "Started training  2  emotion\n",
      "Training took  0.6857340335845947\n",
      "Started training  3  emotion\n",
      "Training took  2.044170618057251\n",
      "Started training  4  emotion\n",
      "Training took  1.728360891342163\n",
      "Started training  5  emotion\n",
      "Training took  0.5480673313140869\n"
     ]
    }
   ],
   "source": [
    "gmms = []\n",
    "imputers = []\n",
    "for i in range(0,6):\n",
    "    print(\"Started training \", i, \" emotion\")\n",
    "    start = time.time()\n",
    "    gmm = mixture.BayesianGaussianMixture(random_state=42)\n",
    "    data = df_clean.loc[df_clean[170] == i].drop([0, 170], axis=1).values\n",
    "    sample = len(data)#vary this param to learn only on some random part of data\n",
    "    data = data[random.sample(range(0, len(data)), sample)]\n",
    "    gmm.fit(data)\n",
    "    gmms.append(gmm)\n",
    "    imputer = preprocessing.Imputer(strategy=\"mean\")\n",
    "    imputer.fit(data)\n",
    "    imputers.append(imputer)\n",
    "    print(\"Training took \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check models on data from df_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:10<00:00, 586.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "X = []\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for i in range(0,6):\n",
    "    data = df_clean.loc[df_clean[170] == i].drop(0, axis=1)\n",
    "    data = data.values[random.sample(range(0, len(data)), 10000)]\n",
    "    for row in data:\n",
    "        X.append(row[:-1])\n",
    "        y_true.append(row[-1])\n",
    "\n",
    "for item in tqdm(X):\n",
    "    y_pred.append(max([(i, gmm.score(item.reshape(1, -1))) for i,gmm in enumerate(gmms)], key=lambda x: x[1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.83349562,  0.9437751 ,  0.94779116,  0.91650485,  0.88865979,\n",
       "         0.9775739 ]),\n",
       " array([ 0.856,  0.94 ,  0.944,  0.944,  0.862,  0.959]),\n",
       " array([ 0.84459793,  0.94188377,  0.94589178,  0.93004926,  0.8751269 ,\n",
       "         0.96819788]),\n",
       " array([1000, 1000, 1000, 1000, 1000, 1000]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_recall_fscore_support(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check models in couple with imputers on part of data from whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [10:23<00:00, 96.29it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for i in range(0,6):\n",
    "    data = train.loc[train[170] == i].drop(0, axis=1)\n",
    "    data = data.values[random.sample(range(0, len(data)), 10000)]\n",
    "    for row in data:\n",
    "        X.append(row[:-1])\n",
    "        y_true.append(row[-1])\n",
    "        \n",
    "for item in tqdm(X):\n",
    "    datas = []\n",
    "    scores = []\n",
    "    for im in imputers:\n",
    "        datas.append(im.transform(item.reshape(1, -1)))\n",
    "    for row in datas:\n",
    "        scores.append(max([(i, gmm.score(row.reshape(1, -1))) for i,gmm in enumerate(gmms)], key=lambda x: x[1]))\n",
    "    \n",
    "    y_pred.append(max(scores, key=lambda x: x[1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.60023481,  0.65299777,  0.79411364,  0.75646552,  0.62920824,\n",
       "         0.61651479]),\n",
       " array([ 0.6135,  0.672 ,  0.5828,  0.8073,  0.6747,  0.663 ]),\n",
       " array([ 0.60679492,  0.66236262,  0.67224177,  0.7810565 ,  0.65116055,\n",
       "         0.63891298]),\n",
       " array([10000, 10000, 10000, 10000, 10000, 10000]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_true, y_pred)\n",
    "metrics.precision_recall_fscore_support(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(models_save_folder, \"gmms\"), \"wb\") as f:\n",
    "    pickle.dump(gmms, f)\n",
    "    \n",
    "with open(os.path.join(models_save_folder, \"imputers\"), \"wb\") as f:\n",
    "    pickle.dump(imputers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, make predictions on test set. This is done in the same manner as per training set. \n",
    "Refactor needed - this code duplicates train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/141 [00:00<?, ?it/s]\u001b[A\n",
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 141/141 [1:38:30<00:00, 41.92s/it]\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "eyes_len = 6\n",
    "kinect_len = 27\n",
    "audio_len = 36\n",
    "face_len = 100\n",
    "\n",
    "results = []\n",
    "files = []\n",
    "for item in os.listdir(os.path.join(test_data, \"eyes\")):\n",
    "    files.append(item)\n",
    "for item in os.listdir(os.path.join(test_data, \"kinect\")):\n",
    "    files.append(item)\n",
    "for item in os.listdir(os.path.join(test_data, \"audio\")):\n",
    "    files.append(item)\n",
    "for item in os.listdir(os.path.join(test_data, \"face_nn\")):\n",
    "    files.append(item)\n",
    "    \n",
    "files = list(set(files))\n",
    "\n",
    "for file in tqdm(files):    \n",
    "    predictions_file = os.path.join(predictions_data, file)\n",
    "    if not os.path.exists(predictions_file):\n",
    "        continue\n",
    "        \n",
    "    eyes_path = os.path.join(test_data, \"eyes\", file)\n",
    "    kinect_path = os.path.join(test_data, \"kinect\", file)\n",
    "    audio_path = os.path.join(test_data, \"audio\", file)\n",
    "    face_nn_path = os.path.join(test_data, \"face_nn\", file)\n",
    "    \n",
    "    eyes_data = get_data_from_file(eyes_path)\n",
    "    audio_data = get_data_from_file(audio_path)\n",
    "    face_nn_data = get_data_from_file(face_nn_path)\n",
    "    kinect_data = get_data_from_file(kinect_path)\n",
    "    predictions = genfromtxt(predictions_file, delimiter=',', skip_header=True)[:, 0:7]\n",
    "    arr = []\n",
    "    \n",
    "    if eyes_data is not None:\n",
    "        arr.append(eyes_data)\n",
    "    if audio_data is not None:\n",
    "        arr.append(audio_data)\n",
    "    if face_nn_data is not None:\n",
    "        arr.append(face_nn_data)\n",
    "    if kinect_data is not None:\n",
    "        arr.append(kinect_data)\n",
    "    arr.append(predictions)    \n",
    "    \n",
    "    max_len_arr = max(arr, key=lambda x:len(x))\n",
    "    max_len = len(max_len_arr)\n",
    "    \n",
    "    if eyes_data is None:\n",
    "        eyes_data = np.hstack((max_len_arr[:, 0].reshape(max_len, 1), np.full((max_len, eyes_len), None)))\n",
    "    if audio_data is None:\n",
    "        audio_data = np.hstack((max_len_arr[:, 0].reshape(max_len, 1), np.full((max_len, audio_len), None)))\n",
    "    if face_nn_data is None:\n",
    "        face_nn_data = np.hstack((max_len_arr[:, 0].reshape(max_len, 1), np.full((max_len, face_len), None)))\n",
    "    if kinect_data is None:\n",
    "        kinect_data = np.hstack((max_len_arr[:, 0].reshape(max_len, 1), np.full((max_len, kinect_len), None)))\n",
    "        \n",
    "    eyes_data, _ = extend(eyes_data, max_len_arr)\n",
    "    audio_data, _ = extend(audio_data, max_len_arr)\n",
    "    face_nn_data, _ = extend(face_nn_data, max_len_arr)\n",
    "    kinect_data, _ = extend(kinect_data, max_len_arr)\n",
    "\n",
    "    whole = np.hstack((eyes_data, kinect_data[:, 1:], audio_data[:, 1:], face_nn_data[:, 1:]))\n",
    "    \n",
    "    pred = []\n",
    "    for item in whole[:, 1:]:\n",
    "        datas = []\n",
    "        scores = []\n",
    "        for im in imputers:\n",
    "            datas.append(im.transform(item.reshape(1, -1)))\n",
    "        for row in datas:\n",
    "            scores.append(max([(i, gmm.score(row.reshape(1, -1))) for i,gmm in enumerate(gmms)], key=lambda x: x[1]))\n",
    "\n",
    "        pred.append(max(scores, key=lambda x: x[1])[0])\n",
    "    results.append([file, pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(preds_save_path, \"wb\") as f:\n",
    "    pickle.dump(results, f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to corresponding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "    file = item[0]\n",
    "    preds = item[1]\n",
    "    path=os.path.join(predictions_data, file)\n",
    "    path_save=os.path.join(predictions_save_folder, file)\n",
    "    r = csv.reader(open(path))\n",
    "    lines = [l for l in r]\n",
    "    \n",
    "    for i, line in enumerate(lines[1:]):\n",
    "        line[1:7] = [0,0,0,0,0,0]\n",
    "        line[preds[i] + 1] = str(1)\n",
    "        line[0] = \"{0:.2f}\".format(float(line[0]))\n",
    "        \n",
    "    writer = csv.writer(open(path_save, 'w'))\n",
    "    writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate that predictions can pass submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 id4ac8ae09.csv 3065 3280\n"
     ]
    }
   ],
   "source": [
    "for item in results:\n",
    "    file = item[0]\n",
    "    preds = item[1]\n",
    "    path=os.path.join(predictions_save_folder, file)\n",
    "    path_truth=os.path.join(predictions_data, file)\n",
    "    r = csv.reader(open(path))\n",
    "    lines = [l for l in r]\n",
    "    r = csv.reader(open(path_truth))\n",
    "    lines_true = [l for l in r]\n",
    "    \n",
    "    # all rows are predicted and not missed\n",
    "    if(len(lines) != len(lines_true)):\n",
    "        print(1, file, len(lines), len(lines_true))\n",
    "    \n",
    "    # all rows have timestamps + emotions columns\n",
    "    for item in lines:\n",
    "        if(len(item) != 7):\n",
    "            print(2, file)\n",
    "            \n",
    "    # rows has exactly 1 emotion set\n",
    "    for item in lines[1:]:\n",
    "        if(sum([int(i) for i in item[1:7]]) != 1):\n",
    "            print(3, file)\n",
    "            \n",
    "    # timestamps are not messed up\n",
    "    time = np.array(lines)[1:, 0]   \n",
    "    for i in range(0, len(time) - 2):\n",
    "        if \"{0:.2f}\".format(float(time[i+1]) - float(time[i])) != \"0.01\":\n",
    "            print(i, file)\n",
    "            break\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
